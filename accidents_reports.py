# -*- coding: utf-8 -*-
"""ACCIDENTS REPORTS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jh51XG9NpqFg4-4NrUH_eMFXgeNDNSdl
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv(r"/content/drive/MyDrive/ML CRASH REPORT PROJ/Crash_Reporting_-_Drivers_Data.csv",dtype={1: str})
df

df1=df.copy()

"""# **Data Structure**"""

df1.head()

df1.shape

df1.info()

missing_percent=df1.isnull().mean()*100

missing_percent

"""# **Considering columns with more than 50% missing values are often less useful**"""

# Dropping identified columns
droping_columns = [
    'Report Number', 'Local Case Number', 'Off-Road Description',
    'Municipality', 'Related Non-Motorist', 'Non-Motorist Substance Abuse',
    'Circumstance', 'Person ID', 'Vehicle ID', 'Latitude', 'Longitude'
]

df1_cleaned = df1.drop(columns=droping_columns)

df1_cleaned.head()

df1_cleaned.info()

df1_cleaned.isnull().sum()

df1_cleaned.info()

"""# **Imputing null values using KNNimputer**"""

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.preprocessing import OrdinalEncoder

n_neighbors = int(np.sqrt(len(df1_cleaned)))
n_neighbors

object_cols = df1_cleaned.select_dtypes(include=['object']).columns
numeric_cols = df1_cleaned.select_dtypes(include=['number']).columns

# Encoding categorical columns
encoder = OrdinalEncoder()
df1_cleaned[object_cols] = encoder.fit_transform(df1_cleaned[object_cols])

#imputing null values
imputer = KNNImputer(n_neighbors=351)
df1_imputed = imputer.fit_transform(df1_cleaned)

# Create a DataFrame from the imputed data
df1_imputed = pd.DataFrame(df1_imputed, columns=df1_cleaned.columns)

# Spliting the imputed data back into numeric and categorical
imputed_numeric = df1_imputed[numeric_cols]
imputed_categorical = df1_imputed[object_cols]

# Convert back to DataFrame and reverse encoding for categorical columns
imputed_categorical = pd.DataFrame(encoder.inverse_transform(imputed_categorical), columns=object_cols)

df1_final_imputed = pd.concat([imputed_numeric, imputed_categorical], axis=1)

print(df1_final_imputed)

csv_file_path = "/content/drive/MyDrive/Colab Notebooks/IMPUTED DATA/df1_final_imputed.csv"
df1_final_imputed.to_csv(csv_file_path, index=False)

print(f"Imputed DataFrame saved to {csv_file_path}")

"""# **Data Frame After imputing null values**"""

import numpy as np
import pandas as pd
df2=pd.read_csv(r"/content/drive/MyDrive/Colab Notebooks/IMPUTED DATA/df1_final_imputed.csv")

df2

df2["Speed Limit"].value_counts()

df2.isnull().sum()

df2.describe(include="number")

df2.describe(include="object").T

"""#**Duplicates**"""

dup_rows=df2[df2.duplicated()].index

dup_rows

df2.drop_duplicates(inplace=True)

df2.duplicated().sum()

df2.reset_index(drop=True,inplace=True)

df2

"""# **Because of the wrong entry data in the Vehicle Year i am considering the Vehicle Year between 1990 and 2024**

"""

df2=df2[df2["Vehicle Year"].between(1990,2024)]

df2



for i in df2.columns:
  print(i,df2[i].nunique())

df2.drop(columns="Location",inplace=True)
df2.drop(columns="Cross-Street Type",inplace=True)
df2.drop(columns="Cross-Street Name",inplace=True)

from datetime import datetime
# Converting 'Crash Date/Time' to datetime
df2['Crash Date/Time'] = pd.to_datetime(df2['Crash Date/Time'],format='%m/%d/%Y %I:%M:%S %p')

# Extracting month and year from 'Crash Date/Time'
df2.loc[:, 'Crash_Month'] = df2['Crash Date/Time'].dt.month
df2.loc[:, 'Crash_Year'] = df2['Crash Date/Time'].dt.year

df2.drop(columns="Crash Date/Time",inplace=True)

df2.describe(include="number")

df2.describe(include="object").T

df2.info()

import seaborn as sns
import matplotlib.pyplot as plt
sns.countplot(x='Crash_Month', data=df2,palette='viridis')
plt.show()

"""## **Insight**:
#### More number of crashes occurred in the month of october

# **Non Visualization Analysis**

### **How Driver substance abuse contributed Injury severity:**
"""

substance_abuse_injury_severity = df2.groupby(['Driver Substance Abuse', 'Injury Severity']).size().unstack(fill_value=0)
substance_abuse_injury_severity

"""## **Statistical Analysis**"""

from scipy.stats import chi2_contingency

# Chi-square test
chi2, p, dof, ex = chi2_contingency(substance_abuse_injury_severity)
print(f"Chi2: {chi2}, p-value: {p}")

"""## **Insights**
#### 1)There is a significant relationship between "Driver Substance Abuse" and "Injury Severity"
#### 2)Because the observed differences in injury severity across different substance abuse categories are far greater than what would be expected by chance.

## **Recommondations:**
### 1)**Education and Awareness Programs:**
Launch campaigns to educate drivers about the dangers of driving under the influence of substances, emphasizing the risk of severe injuries in crashes.
### 2)**Stricter Enforcement and Penalties**:
Push for tougher enforcement of DUI(Driver under influence) laws and penalties to discourage substance-impaired driving and reduce severe injury accidents.

## **How Equipment problems contributed injury severity**
"""

#using cross tab
Equip_prob=pd.crosstab(df2['Equipment Problems'], df2['Injury Severity'], margins=True)
Equip_prob

"""## **Insights**:
#### 1)The data suggests that while most incidents do not result in apparent injuries, equipment problems can still play a significant role in the severity of crashes.
"""

from scipy.stats import chi2_contingency
# Chi-square test
chi2, p, dof, ex = chi2_contingency(Equip_prob)
print(f"Chi2: {chi2}, p-value: {p}")

"""## **Insight:**
#### There is a statistically significant association between equipment problems and injury severity.

## **Recommendations:**
##### 1)**Enhance Airbag Reliability**: Invest in R&D and enforce quality control to ensure airbags deploy correctly during crashes.

##### 2)**Improve Seat Belt Usage and Design**: Implement educational campaigns and develop advanced seat belt technologies to prevent misuse and enhance effectiveness.

##### 3)**Addressing Unknown Equipment Problems**: Conduct detailed investigations to accurately identify and categorize unspecified equipment problems.

## **How weather impacts injuries**
"""

df2[(df2["Weather"]== "RAINING") & (df2["Injury Severity"]=="FATAL INJURY")][["Weather","Speed Limit","Driver Substance Abuse","Injury Severity"]]

"""## **Insights:**
#### 1)**Weather**: All incidents occurred in rainy conditions, suggesting a potential correlation between rain and fatal injuries.
#### 2)**Speed Limit**: Higher speeds (40.0 and 50.0 mph) were more frequent, indicating a possible influence of speed on accident severity.
#### 3)**Driver Substance Abuse**: Alcohol and medication presence were noted in several incidents, highlighting a significant factor in fatal accidents.
#### 4)**Injury Severity:** All incidents resulted in fatal injuries, indicating a critical outcome in each case.

## **Recommendations:**
####**Enhance Safety Measures During Rain:**

Implement stricter speed limit enforcement and improve visibility with better signage and lighting during rainy conditions to reduce accidents.
#### **Promote Driver Education and Awareness:**

Launch targeted campaigns to educate drivers on the dangers of driving under the influence and the critical importance of adhering to speed limits, especially in adverse weather conditions
"""

df2[(df2["Weather"]== "SNOW") & (df2["Injury Severity"]=="SUSPECTED SERIOUS INJURY")][["Weather","Speed Limit","Driver Substance Abuse","Injury Severity"]]

"""## Insights:
#### 1)**Weather**: All incidents occurred in snowy conditions, indicating a potential correlation between snow and suspected serious injuries.
#### 2)**Speed Limit**: Speeds ranged from 35.0 to 60.0 mph, with higher speeds (60.0 mph) involved in incidents, suggesting speed as a contributing factor.
#### 3)**Driver Substance Abuse**: Alcohol presence was noted in one incident, highlighting a potential risk factor in snowy conditions.
#### 4)**Injury Severity**: All incidents resulted in suspected serious injuries, indicating significant consequences despite not being fatal.

## **Recommendations:**
#### **Enhance Winter Driving Preparedness:**
Implement winter-specific road maintenance and de-icing protocols to improve road conditions and reduce accident risks during snowy weather.
"""

df2[(df2["Weather"]== "SEVERE WINDS") & (df2["Injury Severity"]=="POSSIBLE INJURY")][["Weather","Speed Limit","Injury Severity"]]

"""## **Insights:**
#### 1)**Weather**: All incidents occurred in severe winds, indicating a consistent environmental factor.
#### 2)**Speed Limit**: Speeds varied between 15.0 and 55.0 mph, with higher speeds (55.0 mph) involved in one incident, suggesting potential risk associated with higher speeds in severe wind conditions.
#### 3)**Injury Severity**: All incidents resulted in possible injuries, highlighting the impact of severe winds on accident outcomes.

## **Recommendations:**
#### **Enhance Road Safety Protocols During Severe Winds:**
Implement real-time weather monitoring systems to alert drivers and authorities about severe wind conditions, enabling proactive measures.

# **Crash Months**
"""

df2.groupby(["Crash_Month"])["Injury Severity"].value_counts().unstack()

"""# **Insights:**
### **Monthly Trends in Fatal Injuries:**

#### The highest number of fatal injuries occurred in July (19).
#### The lowest number of fatal injuries occurred in January (6).
### **Monthly Trends in Suspected Serious Injuries:**

#### The highest number of suspected serious injuries occurred in May (141).
#### The lowest number of suspected serious injuries occurred in January (83).
### **Correlation Between Fatal and Serious Injuries:**
#### The data shows some months where both types of injuries are relatively high (e.g., July and September), suggesting potential periods of increased risk.

# **Feature selection through feature engineering.**
"""

y = df2["Injury Severity"]
x= df2.drop(["Injury Severity"], axis=1)

categorical_columns = x.select_dtypes(include=['object']).columns.values
numerical_cols = x.select_dtypes(include=['number']).columns.values

chi2_results = []

# Performing Chi-Square test for each categorical feature
for column in categorical_columns:
    contingency_table = pd.crosstab(df2[column], y)
    chi2, p, dof, ex = chi2_contingency(contingency_table)
    chi2_results.append((column, chi2, p, dof))

for column, chi2, p, dof in chi2_results:
    print(f"Column: {column} | Chi2: {chi2:.2f} | p-value: {p:.4f} | Degrees of freedom: {dof}")

"""## **From the statistical analysis based on chi2 and degrees of freedom and from my intution I found the important features for predicting the target variable "Injury Severity"**"""

!pip install category_encoders

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold, train_test_split, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from category_encoders import BinaryEncoder
import numpy as np
from tqdm.notebook import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report,confusion_matrix
import time
from imblearn.over_sampling import ADASYN

df3=df2.sample(20000,random_state=100)

# Features and target
y = df3["Injury Severity"]
x = df3[["ACRS Report Type", "Collision Type", "Surface Condition", "Driver Substance Abuse",
         "Driver At Fault", "Vehicle Damage Extent", "Vehicle Body Type",
         "Equipment Problems", "Speed Limit", "Vehicle Year", "Crash_Month"]]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, random_state=42)

numerical_cols = x.select_dtypes(include=['number']).columns
categorical_cols = x.select_dtypes(include=['object']).columns

numerical_cols

categorical_cols

binary_encoder = BinaryEncoder()
x_train_categorical_encoded = binary_encoder.fit_transform(x_train[categorical_cols])
x_test_categorical_encoded = binary_encoder.transform(x_test[categorical_cols])

scaler = StandardScaler()
x_train_numerical_scaled = scaler.fit_transform(x_train[numerical_cols])
x_test_numerical_scaled = scaler.transform(x_test[numerical_cols])

x_train_numerical_scaled_df = pd.DataFrame(x_train_numerical_scaled, columns=numerical_cols)
x_test_numerical_scaled_df = pd.DataFrame(x_test_numerical_scaled, columns=numerical_cols)
x_train_categorical_encoded_df = pd.DataFrame(x_train_categorical_encoded, columns=x_train_categorical_encoded.columns)
x_test_categorical_encoded_df = pd.DataFrame(x_test_categorical_encoded, columns=x_test_categorical_encoded.columns)

# Reset index of Pandas DataFrames
x_train_numerical_scaled_df.reset_index(drop=True, inplace=True)
x_test_numerical_scaled_df.reset_index(drop=True, inplace=True)
x_train_categorical_encoded_df.reset_index(drop=True, inplace=True)
x_test_categorical_encoded_df.reset_index(drop=True, inplace=True)

# Concatenate transformed numerical and categorical data
x_train_preprocessed = pd.concat([x_train_numerical_scaled_df, x_train_categorical_encoded_df], axis=1)
x_test_preprocessed = pd.concat([x_test_numerical_scaled_df, x_test_categorical_encoded_df], axis=1)

binary_encoder = BinaryEncoder()
scaler = StandardScaler()

numerical_transformer = Pipeline(steps = [('scaling',scaler)])
categorical_transformer = Pipeline(steps=[('encoding',binary_encoder)])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer,categorical_cols )])

pipe = Pipeline([('preprocessor',preprocessor)])

"""# **Using ADASYN for imbalnce data**"""

adasyn = ADASYN(random_state=42)
x_train_resample, y_train_resample = adasyn.fit_resample(x_train_preprocessed, y_train)

models = {
    'knn': KNeighborsClassifier(),
    'decision_tree': DecisionTreeClassifier(),
    'logistic_regression': LogisticRegression(max_iter=1000),
    'random_forest': RandomForestClassifier(),
}

param_distributions = {
    'knn': {
        'n_neighbors': np.arange(1, 10),
        'weights': ['uniform', 'distance']
    },
    'decision_tree': {
        'max_depth': np.arange(3, 7),
        'min_samples_split': np.arange(2, 7)
    },
    'logistic_regression': {
        'penalty': ['l2'],
        'C': [0.1, 0.2, 0.3]
    },
    'random_forest': {
        'n_estimators': [100, 200],
        'max_depth': np.arange(1, 5),
    },
}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

best_models = {}

for model_name, model in tqdm(models.items(), desc="Model Progress"):
    search = RandomizedSearchCV(estimator=model,
                                param_distributions=param_distributions[model_name],
                                n_iter=10,
                                scoring='accuracy',
                                cv=skf,
                                random_state=42,
                                verbose=2)

    search.fit(x_train_resample, y_train_resample)

    best_models[model_name] = search.best_estimator_

    print(f"Best hyperparameters for {model_name}: {search.best_params_}")

for model_name, model in best_models.items():
    print(f"Evaluating {model_name}:")

    # Training scores
    train_start_time = time.time()

    model.fit(x_train_resample, y_train_resample)

    train_end_time = time.time()

    train_time = train_end_time - train_start_time
    print(train_time)

    train_pred = model.predict(x_train_resample)
    train_accuracy = accuracy_score(y_train_resample, train_pred)
    train_report = classification_report(y_train_resample, train_pred)
    print(f"Training Accuracy: {train_accuracy:.4f}")

    # Testing scores
    test_start_time = time.time()

    test_pred = model.predict(x_test_preprocessed)

    test_end_time = time.time()

    test_time = test_end_time - test_start_time
    print(test_time)

    test_accuracy = accuracy_score(y_test, test_pred)
    test_report = classification_report(y_test, test_pred)
    print(f"Testing Accuracy: {test_accuracy:.4f}")

    print("-----------------------------------------")

final_pipeline = Pipeline(steps=[
    ('classifier', DecisionTreeClassifier(min_samples_split=2, max_depth=6))
])

final_pipeline.fit(x_train_resample, y_train_resample)

y_pred = final_pipeline.predict(x_test_preprocessed)
final_accuracy = accuracy_score(y_test, y_pred)
final_report = classification_report(y_test, y_pred)

print("Final pipeline fitted.")
print(f"Final Testing Accuracy: {final_accuracy:.4f}")
print(final_report)

"""# **observation:**
## **Model Efficiency**: The decision tree was trained in 0.30 seconds and tested in 0.004 seconds, demonstrating quick processing times.
## **Training Performance**: Achieved a training accuracy of 66.05%, indicating  moderate fitting to the training data.
## **Testing Performance**: Attained a testing accuracy of 73.87%, suggesting good generalization to new, unseen data.



"""

cm = confusion_matrix(y_test, y_pred)

# Normalize confusion matrix
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

print("Confusion Matrix (Normalized):")
print(cm_normalized)

# Plot normalized confusion matrix
plt.figure(figsize=(5,5))
sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=final_pipeline.classes_, yticklabels=final_pipeline.classes_)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Normalized Confusion Matrix')
plt.show()

"""# **pipeline only for deployment**"""

classifier = DecisionTreeClassifier(min_samples_split=2, max_depth=6)
final_pl = Pipeline(steps=[
    ('preprocessor', pipe),
    ('classifier', classifier)
])

# Fitting the pipeline on the training data
final_pl.fit(x_train, y_train)

"""# **Creating Joblib**"""

import joblib

joblib.dump(final_pl, 'final_pl.pkl')

model = joblib.load('final_pl.pkl')

model

joblib.dump(model, 'final_pl.pkl')

joblib.dump(model, '/content/drive/MyDrive/Colab Notebooks/text_dataframe/final_pl.pkl')

model = joblib.load("/content/drive/MyDrive/Colab Notebooks/text_dataframe/final_pl.pkl")